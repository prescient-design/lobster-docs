{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2ffd61",
   "metadata": {},
   "source": [
    "# Training a LBSTER Model from Scratch\n",
    "\n",
    "This tutorial demonstrates how to train a LBSTER protein language model from scratch using your own dataset. We'll cover data preparation, model configuration, training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535c08f",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, make sure LBSTER is installed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74277a33",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45267185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91436160",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from lobster.data import DataFrameLightningDataModule\n",
    "from lobster.tokenization import PmlmTokenizerTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f82e4",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Your Data\n",
    "\n",
    "You'll need protein sequences for training. For this tutorial, we'll create a small synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200c3a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create a synthetic dataset of protein sequences\n",
    "def generate_synthetic_data(n_samples=1000, min_len=50, max_len=200):\n",
    "    \"\"\"Generate synthetic protein sequences for demonstration.\"\"\"\n",
    "    import random\n",
    "    \n",
    "    # Standard amino acids\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    \n",
    "    sequences = []\n",
    "    for _ in range(n_samples):\n",
    "        # Random sequence length\n",
    "        length = random.randint(min_len, max_len)\n",
    "        # Generate random sequence\n",
    "        seq = ''.join(random.choice(amino_acids) for _ in range(length))\n",
    "        sequences.append(seq)\n",
    "    \n",
    "    return pd.DataFrame({'sequence': sequences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "df = generate_synthetic_data(n_samples=1000)\n",
    "print(f\"Generated {len(df)} protein sequences\")\n",
    "print(f\"Example sequence: {df['sequence'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d17b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for use with hydra config\n",
    "df.to_csv('synthetic_proteins.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066920e",
   "metadata": {},
   "source": [
    "## Step 2: Create a Configuration File\n",
    "\n",
    "LBSTER uses Hydra for configuration. Let's create a basic config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54fabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a simple training config to a file\n",
    "train_config = \"\"\"\n",
    "# @package _global_\n",
    "\n",
    "# paths config\n",
    "paths:\n",
    "  root_dir: \"./\"\n",
    "\n",
    "# setup config \n",
    "setup:\n",
    "  _target_: hydra.utils.instantiate_at_run\n",
    "  \n",
    "# data config\n",
    "data:\n",
    "  _target_: lobster.data.DataFrameLightningDataModule\n",
    "  data: \n",
    "    _target_: pandas.read_csv\n",
    "    filepath_or_buffer: synthetic_proteins.csv\n",
    "  columns: [\"sequence\"]\n",
    "  batch_size: 8\n",
    "  max_length: 256\n",
    "  num_workers: 2\n",
    "  transform_fn:\n",
    "    _target_: lobster.tokenization.PmlmTokenizerTransform\n",
    "    padding: \"max_length\"\n",
    "    truncation: true\n",
    "    max_length: 256\n",
    "    tokenizer_dir: \"pmlm_tokenizer\"\n",
    "\n",
    "# model config\n",
    "model:\n",
    "  _target_: lobster.model.LobsterPMLM\n",
    "  max_length: 256\n",
    "  hidden_dim: 128\n",
    "  n_layer: 2\n",
    "  n_head: 4\n",
    "  dim_feedforward: 512\n",
    "  learning_rate: 1e-4\n",
    "  weight_decay: 0.01\n",
    "  warmup_steps: 100\n",
    "\n",
    "# trainer config\n",
    "trainer:\n",
    "  _target_: lightning.pytorch.Trainer\n",
    "  max_epochs: 5\n",
    "  accelerator: auto\n",
    "  devices: 1\n",
    "  log_every_n_steps: 10\n",
    "  val_check_interval: 0.5\n",
    "  \n",
    "# logger config\n",
    "logger:\n",
    "  _target_: lightning.pytorch.loggers.TensorBoardLogger\n",
    "  save_dir: \"./logs\"\n",
    "  name: \"lobster-training\"\n",
    "  \n",
    "# callbacks config\n",
    "callbacks:\n",
    "  checkpoint_callback:\n",
    "    _target_: lightning.pytorch.callbacks.ModelCheckpoint\n",
    "    monitor: \"val_loss\"\n",
    "    mode: \"min\"\n",
    "    save_top_k: 1\n",
    "    save_last: true\n",
    "    dirpath: \"./checkpoints\"\n",
    "    filename: \"{epoch}-{val_loss:.2f}\"\n",
    "  early_stopping:\n",
    "    _target_: lightning.pytorch.callbacks.EarlyStopping\n",
    "    monitor: \"val_loss\"\n",
    "    patience: 3\n",
    "    mode: \"min\"\n",
    "    \n",
    "# don't actually run training in this example\n",
    "dryrun: true\n",
    "\n",
    "# don't test after training\n",
    "run_test: false\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the config to a YAML file\n",
    "with open('train_config.yaml', 'w') as f:\n",
    "    f.write(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221a429",
   "metadata": {},
   "source": [
    "## Step 3: Model Configuration\n",
    "\n",
    "Let's understand how to configure the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f89b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print important model configuration parameters\n",
    "print(\"Key model configuration parameters:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"hidden_dim: Size of the hidden layers\")\n",
    "print(\"n_layer: Number of transformer layers\")\n",
    "print(\"n_head: Number of attention heads\")\n",
    "print(\"dim_feedforward: Size of the feedforward network in transformer layers\")\n",
    "print(\"learning_rate: Initial learning rate for optimizer\")\n",
    "print(\"weight_decay: L2 regularization\")\n",
    "print(\"warmup_steps: Number of warmup steps for learning rate scheduler\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e5a96",
   "metadata": {},
   "source": [
    "## Step 4: Training Process\n",
    "\n",
    "We'll use the hydra CLI to train the model, but in this notebook we'll just illustrate the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a5617",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Here's how you would execute the training from the command line:\n",
    "print(\"To train the model, run this command from the terminal:\")\n",
    "print(\"lobster_train -cn train_config\")\n",
    "print(\"\\nFor larger datasets or more complex models, you might want to add more options:\")\n",
    "print(\"lobster_train -cn train_config trainer.max_epochs=10 trainer.devices=2 model.learning_rate=5e-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9091f2",
   "metadata": {},
   "source": [
    "## Step 5: Simulated Training Process\n",
    "\n",
    "Let's simulate what happens during training to understand the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1056b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Simple simulation of training process (not actually training)\n",
    "def simulate_training_process():\n",
    "    print(\"\\nSimulating training process:\")\n",
    "    print(\"1. Loading configuration...\")\n",
    "    print(\"2. Preparing data module...\")\n",
    "    print(\"3. Initializing model...\")\n",
    "    print(\"4. Setting up trainer with callbacks...\")\n",
    "    \n",
    "    # Simple training loss simulation\n",
    "    epochs = 5\n",
    "    train_losses = [2.5 - i * 0.4 + 0.1 * np.random.randn() for i in range(epochs)]\n",
    "    val_losses = [2.3 - i * 0.35 + 0.15 * np.random.randn() for i in range(epochs)]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Train loss: {train_losses[epoch]:.4f}\")\n",
    "        print(f\"  Val loss: {val_losses[epoch]:.4f}\")\n",
    "    \n",
    "    print(\"\\n5. Saving best checkpoint...\")\n",
    "    print(\"6. Training complete!\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56104774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_losses, val_losses = simulate_training_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulated training process\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-o', label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-o', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Simulated Training Process')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbdbb5",
   "metadata": {},
   "source": [
    "## Step 6: Model Evaluation\n",
    "\n",
    "After training, you should evaluate your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model evaluation metrics:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Perplexity: Exponentiated average negative log-likelihood per token\")\n",
    "print(\"Loss: Negative log-likelihood loss on held-out data\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b33b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated perplexity calculation\n",
    "final_val_loss = val_losses[-1]\n",
    "perplexity = np.exp(final_val_loss)\n",
    "print(f\"Final validation perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab035ae",
   "metadata": {},
   "source": [
    "## Step 7: Using Your Trained Model\n",
    "\n",
    "Once training is complete, you can load and use your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Code to load your trained model:\")\n",
    "print(\"\"\"\n",
    "from lobster.model import LobsterPMLM\n",
    "\n",
    "# Load model from checkpoint\n",
    "model = LobsterPMLM.load_from_checkpoint(\"path/to/best/checkpoint.ckpt\")\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Use for inference\n",
    "sequence = \"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSHGSAQVKGH\"\n",
    "tokens = model.tokenizer(sequence, return_tensors=\"pt\")\n",
    "embeddings = model.model(input_ids=tokens[\"input_ids\"], \n",
    "                          attention_mask=tokens[\"attention_mask\"])\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a2ae7",
   "metadata": {},
   "source": [
    "## Advanced: Training Strategies for Production Models\n",
    "\n",
    "For larger models and datasets, consider these strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAdvanced training strategies:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"1. Gradient Accumulation: For training with effectively larger batch sizes\")\n",
    "print(\"   trainer.accumulate_grad_batches=4\")\n",
    "print()\n",
    "print(\"2. Mixed Precision Training: For faster training and lower memory usage\")\n",
    "print(\"   trainer.precision=16\")\n",
    "print()\n",
    "print(\"3. Distributed Training: For multi-GPU setups\")\n",
    "print(\"   trainer.strategy='ddp' trainer.devices=4\")\n",
    "print()\n",
    "print(\"4. Checkpoint Resuming: To continue training from a checkpoint\")\n",
    "print(\"   trainer.resume_from_checkpoint='path/to/last/checkpoint.ckpt'\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ed51f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've covered:\n",
    "\n",
    "1. Data preparation for training LBSTER models\n",
    "2. Configuration using Hydra\n",
    "3. Model architecture configuration\n",
    "4. The training process\n",
    "5. Evaluation metrics\n",
    "6. Using the trained model\n",
    "7. Advanced training strategies\n",
    "\n",
    "For optimal results with real data, you would typically need:\n",
    "- A larger dataset (>100K sequences)\n",
    "- More compute resources\n",
    "- Longer training time (days to weeks depending on model size)\n",
    "- Careful hyperparameter tuning\n",
    "\n",
    "The LBSTER paper demonstrates that effective protein language models can be trained in as little as 24 GPU hours with the right configuration and data processing pipeline."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
