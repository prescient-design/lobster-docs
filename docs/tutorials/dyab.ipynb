{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DyAb Model: Training and Inference Tutorial\n",
    "# ====================================\n",
    "#\n",
    "# This notebook demonstrates how to use the DataFrameLightningDataModule with the DyAbModel\n",
    "# for training and performing inference on antibody data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Import the necessary lobster modules\n",
    "from lobster.data import DyAbDataFrameLightningDataModule\n",
    "from lobster.tokenization import AminoAcidTokenizerFast\n",
    "from lobster.transforms import TokenizerTransform\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a Sample Dataset\n",
    "\n",
    "For this tutorial, we'll create a synthetic dataset to demonstrate the functionality.\n",
    "In real applications, you would use your own antibody data.\n",
    "\n",
    "The DyAb model is trained to predict the difference between properties (y1 - y2),\n",
    "so we need to create our data accordingly. DyAb is also designed to be trained on\n",
    "sequences that are 1 edit-distance apart, we can ignore this in our toy example, but\n",
    "we will make the sequences all be the same length to ensure we train a reasonable model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data(num_samples=500, seed=42):\n",
    "    \"\"\"Generate synthetic antibody data for demonstration with DyAb model.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create random heavy and light chain sequences\n",
    "    amino_acids = list(\"ARNDCEQGHILKMFPSTWYV\")\n",
    "    \n",
    "    heavy_length = np.random.randint(110, 130, 1)[0]\n",
    "    light_length = np.random.randint(105, 115, 1)[0]\n",
    "    \n",
    "    heavy_chains = []\n",
    "    light_chains = []\n",
    "    pkd_values = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        heavy = ''.join(np.random.choice(amino_acids, heavy_length))\n",
    "        light = ''.join(np.random.choice(amino_acids, light_length))\n",
    "        heavy_chains.append(heavy)\n",
    "        light_chains.append(light)\n",
    "\n",
    "        # Create synthetic pKD values (binding affinity)\n",
    "        # For this toy example, we'll make more A's in the hc and M's in the lc \n",
    "        # mean high affinity\n",
    "        pkd_values.append((heavy.count('A') + light.count('M'))/2 + np.random.normal(0, 0.5, 1)[0])\n",
    "\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'fv_heavy': heavy_chains,\n",
    "        'fv_light': light_chains,\n",
    "        'pKD': pkd_values\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate sample data\n",
    "df = generate_sample_data(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up the DyAbDataFrameLightningDataModule\n",
    "\n",
    "The DyAb model expects data in the form of paired sequences with their corresponding\n",
    "target values, where during training it learns to predict the differences between pairs.\n",
    "\n",
    "Let's set up the data module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer transform\n",
    "tokenizer = AminoAcidTokenizerFast()\n",
    "transform_fn = TokenizerTransform(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "# Initialize the DyAb datamodule\n",
    "dyab_datamodule = DyAbDataFrameLightningDataModule(\n",
    "    data=df,\n",
    "    remove_nulls=True,\n",
    "    transform_fn=transform_fn,\n",
    "    lengths=[0.8, 0.1, 0.1],  # Train, val, test split\n",
    "    batch_size=16,\n",
    "    seed=SEED,\n",
    "    num_workers=4,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# Set up the datamodule\n",
    "dyab_datamodule.prepare_data()\n",
    "dyab_datamodule.setup(stage=\"fit\")\n",
    "\n",
    "# Let's examine what a batch from the dataloader looks like\n",
    "train_dataloader = dyab_datamodule.train_dataloader()\n",
    "for batch in train_dataloader:\n",
    "    sequence1, sequence2, target1, target2 = batch\n",
    "    print(\"Sequence1 shape:\", np.array(sequence1[0]).shape)  # First element of the tuple for heavy chain\n",
    "    print(\"Sequence2 shape:\", np.array(sequence2[0]).shape)  # First element of the tuple for heavy chain\n",
    "    print(\"Target1 shape:\", target1.shape)\n",
    "    print(\"Target2 shape:\", target2.shape)\n",
    "    print(\"Target difference (what model will predict):\", (target1 - target2).mean().item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using the DyAb Model\n",
    "\n",
    "Now we'll initialize and train the DyAb model. We'll use the FlexBERT architecture\n",
    "which is designed to handle protein sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DyAb model\n",
    "from lobster.model import DyAbModel\n",
    "\n",
    "# Initialize the model\n",
    "dyab_model = DyAbModel(\n",
    "    model_name=\"esm2_t6_8M_UR50D\",\n",
    "    embedding_img_size=224,\n",
    "    diff_channel_0=\"diff\",\n",
    "    diff_channel_1=\"diff\",\n",
    "    diff_channel_2=\"diff\"\n",
    ")\n",
    "\n",
    "# Define callbacks for training\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath='checkpoints/',\n",
    "        filename='dyab-{epoch:02d}-{val_loss:.4f}',\n",
    "        save_top_k=3,\n",
    "        monitor='val/loss',\n",
    "        mode='min'\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val/loss',\n",
    "        patience=10,\n",
    "        mode='min'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=callbacks,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model\n",
    "\n",
    "Now let's train the DyAb model using our datamodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(dyab_model, datamodule=dyab_datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the Model\n",
    "\n",
    "Let's evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_results = trainer.test(dyab_model, datamodule=dyab_datamodule)\n",
    "print(f\"Test results: {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings\n",
    "for emb in dyab_model.embedding_cache:\n",
    "    print(dyab_model.embedding_cache[emb].shape)\n",
    "    if dyab_model.embedding_cache[emb].dim() == 3:\n",
    "        plt.imshow(dyab_model.embedding_cache[emb][0].cpu())\n",
    "    else:\n",
    "        plt.imshow(dyab_model.embedding_cache[emb].cpu())\n",
    "    plt.title('antiberty_emb')\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "for batch in dyab_datamodule.train_dataloader():\n",
    "    sequences1, sequences2, y1, y2 = batch\n",
    "    actual_diff = (y1 - y2).float()\n",
    "\n",
    "    if len(sequences1) == 2:  # concat multiple chains\n",
    "        sequences1 = [s1 + \".\" + s2 for s1, s2 in zip(*sequences1)]\n",
    "    if len(sequences2) == 2:  # concat multiple chains\n",
    "        sequences2 = [s1 + \".\" + s2 for s1, s2 in zip(*sequences2)]\n",
    "\n",
    "    for seq1, seq2 in zip(sequences1, sequences2):\n",
    "        if seq1 not in dyab_model.embedding_cache:\n",
    "            with torch.inference_mode():\n",
    "                hidden_states = dyab_model.model.sequences_to_latents([seq1])[-2].to(device).float()\n",
    "            dyab_model.embedding_cache[seq1] = hidden_states\n",
    "        if seq2 not in dyab_model.embedding_cache:\n",
    "            with torch.inference_mode():\n",
    "                hidden_states = dyab_model.model.sequences_to_latents([seq2])[-2].to(device).float()\n",
    "            dyab_model.embedding_cache[seq2] = hidden_states\n",
    "\n",
    "        embeddings1 = torch.concat([dyab_model.embedding_cache[seq] for seq in sequences1], dim=0).to(dyab_model.device)\n",
    "        embeddings2 = torch.concat([dyab_model.embedding_cache[seq] for seq in sequences2], dim=0).to(dyab_model.device)\n",
    "\n",
    "    embedding_image = dyab_model._resize_embeddings(embeddings1, embeddings2)\n",
    "\n",
    "    numpy_img = embedding_image[0].cpu().numpy()\n",
    "    numpy_img = np.transpose(numpy_img, (1, 2, 0))\n",
    "\n",
    "    plt.imshow(numpy_img)\n",
    "    plt.title(\"embedding_diff\")\n",
    "    plt.show()\n",
    "\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize an embedding\n",
    "\n",
    "DyAb uses embeddings generated by another protein language model to predict the difference between two\n",
    "sequences. Let's visualize one of these embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform Inference on New Sequences\n",
    "\n",
    "Now let's demonstrate how to use the trained model for inference on new antibody sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binding_differences(model, new_data):\n",
    "    \"\"\"\n",
    "    Predict binding affinity differences for pairs of antibody sequences.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained DyAb model\n",
    "        new_data: DataFrame with fv_heavy, fv_light, pKD columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predicted differences\n",
    "    \"\"\"\n",
    "    # Create a datamodule for inference\n",
    "    inference_datamodule = DyAbDataFrameLightningDataModule(\n",
    "        data=new_data,\n",
    "        remove_nulls=True,\n",
    "        transform_fn=transform_fn,\n",
    "        lengths=[0, 0, 1],  # All data for inference\n",
    "        batch_size=16,\n",
    "        seed=SEED,\n",
    "        num_workers=4,\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    # Set up the datamodule\n",
    "    inference_datamodule.prepare_data()\n",
    "    inference_datamodule.setup(stage=\"predict\")\n",
    "    \n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    sequence1_data = []\n",
    "    sequence2_data = []\n",
    "    actual_diffs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(inference_datamodule.predict_dataloader()):\n",
    "            sequence1, sequence2, target1, target2 = batch\n",
    "            \n",
    "            preds, _ = model.predict_step(batch, idx)\n",
    "\n",
    "            # y_hat, _ = trainer.predict(model, inference_datamodule)[idx]\n",
    "\n",
    "            # Process through the model\n",
    "            # pred_diff = model(sequence1, sequence2)\n",
    "            actual_diff = (target1 - target2).cpu().numpy()\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            actual_diffs.extend(actual_diff)\n",
    "            \n",
    "            # Store sequence information for reference\n",
    "            for i in range(len(target1)):\n",
    "                sequence1_data.append({\n",
    "                    \"heavy\": sequence1[0][i],\n",
    "                    \"light\": sequence1[1][i],\n",
    "                    \"target\": target1[i].item()\n",
    "                })\n",
    "                sequence2_data.append({\n",
    "                    \"heavy\": sequence2[0][i],\n",
    "                    \"light\": sequence2[1][i],\n",
    "                    \"target\": target2[i].item()\n",
    "                })            \n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        \"Predicted_Difference\": predictions,\n",
    "        \"Actual_Difference\": actual_diffs,\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Generate some new sequences for inference\n",
    "new_data = generate_sample_data(100)\n",
    "\n",
    "# Predict binding affinity differences\n",
    "results_df = predict_binding_differences(dyab_model, new_data)\n",
    "\n",
    "# Display results\n",
    "print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Predictions vs. Actual Differences\n",
    "\n",
    "Let's visualize how well our model's predictions match the actual differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted differences\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(results_df[\"Actual_Difference\"], results_df[\"Predicted_Difference\"], alpha=0.7)\n",
    "plt.plot(\n",
    "    [min(results_df[\"Actual_Difference\"]), max(results_df[\"Actual_Difference\"])], \n",
    "    [min(results_df[\"Actual_Difference\"]), max(results_df[\"Actual_Difference\"])], \n",
    "    'r--'\n",
    ")\n",
    "plt.xlabel('Actual Difference')\n",
    "plt.ylabel('Predicted Difference')\n",
    "plt.title('Predicted vs. Actual Affinity Differences')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate R² score\n",
    "r2 = r2_score(results_df[\"Actual_Difference\"], results_df[\"Predicted_Difference\"])\n",
    "print(f\"R² score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using the Model for Ranking Antibodies\n",
    "\n",
    "One valuable application of the DyAb model is to rank antibodies based on predicted affinities.\n",
    "Let's demonstrate how to use the model for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_antibodies(model, antibody_pool, reference_antibody):\n",
    "    \"\"\"\n",
    "    Rank a pool of antibodies against a reference antibody based on predicted binding affinity.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained DyAb model\n",
    "        antibody_pool: DataFrame with fv_heavy, fv_light, pKD columns for candidate antibodies\n",
    "        reference_antibody: Dict with fv_heavy, fv_light for the reference antibody\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with ranked antibodies\n",
    "    \"\"\"\n",
    "    # Create pairs of reference and candidate antibodies\n",
    "    pairs_data = []\n",
    "    \n",
    "    for _, candidate in antibody_pool.iterrows():\n",
    "        pairs_data.append({\n",
    "            \"fv_heavy\": reference_antibody[\"fv_heavy\"],\n",
    "            \"fv_light\": reference_antibody[\"fv_light\"],\n",
    "            \"pKD\": 0.0,  # Dummy value for reference\n",
    "            \"candidate_heavy\": candidate[\"fv_heavy\"],\n",
    "            \"candidate_light\": candidate[\"fv_light\"],\n",
    "            \"candidate_pKD\": candidate[\"pKD\"]\n",
    "        })\n",
    "    \n",
    "    pairs_df = pd.DataFrame(pairs_data)\n",
    "    \n",
    "    # Create a combined dataset for DyAb format\n",
    "    # We need to duplicate each row and swap the order to get predictions in both directions\n",
    "    combined_data = []\n",
    "    \n",
    "    for _, row in pairs_df.iterrows():\n",
    "        # Reference first, candidate second\n",
    "        combined_data.append({\n",
    "            \"fv_heavy\": row[\"fv_heavy\"],\n",
    "            \"fv_light\": row[\"fv_light\"],\n",
    "            \"pKD\": row[\"pKD\"]\n",
    "        })\n",
    "        \n",
    "        # Candidate first, reference second\n",
    "        combined_data.append({\n",
    "            \"fv_heavy\": row[\"candidate_heavy\"],\n",
    "            \"fv_light\": row[\"candidate_light\"],\n",
    "            \"pKD\": row[\"candidate_pKD\"]\n",
    "        })\n",
    "    \n",
    "    combined_df = pd.DataFrame(combined_data)\n",
    "    \n",
    "    # Create a datamodule\n",
    "    inference_datamodule = DyAbDataFrameLightningDataModule(\n",
    "        data=combined_df,\n",
    "        remove_nulls=True,\n",
    "        transform_fn=transform_fn,\n",
    "        lengths=[0, 0, 1],  # All data for inference\n",
    "        batch_size=16,\n",
    "        seed=SEED,\n",
    "        num_workers=4,\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    # Set up the datamodule\n",
    "    inference_datamodule.prepare_data()\n",
    "    inference_datamodule.setup(stage=\"predict\")\n",
    "    \n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in inference_datamodule.predict_dataloader():\n",
    "            # pred_diff = model(sequence1, sequence2)\n",
    "            preds = dyab_model.predict(batch)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Process predictions (every second prediction is for ref-candidate pair)\n",
    "    ref_candidate_preds = all_preds[::2]\n",
    "    \n",
    "    # Add predictions to the original dataframe\n",
    "    for i, pred in enumerate(ref_candidate_preds):\n",
    "        pairs_df.loc[i, \"predicted_diff_ref_candidate\"] = pred\n",
    "    \n",
    "    # Sort by prediction (higher predicted difference means candidate is better than reference)\n",
    "    ranked_df = pairs_df.sort_values(by=\"predicted_diff_ref_candidate\", ascending=False)\n",
    "    \n",
    "    # Add rank column\n",
    "    ranked_df[\"rank\"] = range(1, len(ranked_df) + 1)\n",
    "    \n",
    "    return ranked_df[[\"rank\", \"candidate_heavy\", \"candidate_light\", \"candidate_pKD\", \"predicted_diff_ref_candidate\"]]\n",
    "\n",
    "# Generate a pool of candidate antibodies\n",
    "candidate_pool = generate_sample_data(100)\n",
    "\n",
    "# Select a reference antibody\n",
    "reference_antibody = {\n",
    "    \"fv_heavy\": df.iloc[0][\"fv_heavy\"],\n",
    "    \"fv_light\": df.iloc[0][\"fv_light\"]\n",
    "}\n",
    "\n",
    "# Rank antibodies\n",
    "ranked_candidates = rank_antibodies(dyab_model, candidate_pool, reference_antibody)\n",
    "\n",
    "# Display top 10 ranked antibodies\n",
    "print(\"Top 10 Ranked Antibodies:\")\n",
    "print(ranked_candidates.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Saving and Loading the Model\n",
    "\n",
    "Demonstrating how to save and load a trained DyAb model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = \"dyab_model.ckpt\"\n",
    "trainer.save_checkpoint(model_path)\n",
    "\n",
    "# Load the model\n",
    "loaded_model = DyAbModel.load_from_checkpoint(model_path)\n",
    "\n",
    "# Verify the loaded model\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a batch from the dataloader\n",
    "    for batch in dyab_datamodule.val_dataloader():\n",
    "        sequence1, sequence2, target1, target2 = batch\n",
    "        \n",
    "        # Make predictions with both models\n",
    "        # original_pred = dyab_model(sequence1, sequence2)\n",
    "        with torch.inference_mode():\n",
    "            output = dyab_model._compute_loss(batch)\n",
    "        assert output is not None\n",
    "        _, original_pred, _ = output\n",
    "        \n",
    "        # loaded_pred = loaded_model(sequence1, sequence2)\n",
    "        with torch.inference_mode():\n",
    "            output = loaded_model._compute_loss(batch)\n",
    "        assert output is not None\n",
    "        _, loaded_pred, _ = output\n",
    "        \n",
    "        # Verify they give the same predictions\n",
    "        print(\"Original model prediction mean:\", original_pred.mean().item())\n",
    "        print(\"Loaded model prediction mean:\", loaded_pred.mean().item())\n",
    "        \n",
    "        # Check if predictions are identical\n",
    "        is_identical = torch.allclose(original_pred, loaded_pred, atol=1e-5)\n",
    "        print(f\"Models give identical predictions: {is_identical}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this tutorial, we've demonstrated how to:\n",
    "\n",
    "1. Prepare antibody data for the DyAb model\n",
    "2. Train a DyAb model using DataFrameLightningDataModule\n",
    "3. Evaluate the model's performance\n",
    "4. Use the model for inference and ranking of antibodies\n",
    "5. Save and load the model\n",
    "\n",
    "The DyAb model is particularly useful for learning pairwise relationships between sequences,\n",
    "which makes it valuable for tasks like antibody optimization, where you want to predict if\n",
    "a modification to a sequence will improve its properties."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
