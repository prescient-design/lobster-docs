{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DyAb Model: Training and Inference Tutorial\n",
    "# ====================================\n",
    "#\n",
    "# This notebook demonstrates how to use the DataFrameLightningDataModule with the DyAbModel\n",
    "# for training and performing inference on antibody data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damianit/Documents/lobster-docs/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Import the necessary lobster modules\n",
    "from lobster.data import DyAbDataFrameLightningDataModule\n",
    "from lobster.tokenization import AminoAcidTokenizerFast\n",
    "from lobster.transforms import TokenizerTransform\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a Sample Dataset\n",
    "\n",
    "For this tutorial, we'll create a synthetic dataset to demonstrate the functionality.\n",
    "In real applications, you would use your own antibody data.\n",
    "\n",
    "The DyAb model is trained to predict the difference between properties (y1 - y2),\n",
    "so we need to create our data accordingly. DyAb is also designed to be trained on\n",
    "sequences that are 1 edit-distance apart, we can ignore this in our toy example, but\n",
    "we will make the sequences all be the same length to ensure we train a reasonable model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data(num_samples=500, seed=42):\n",
    "    \"\"\"Generate synthetic antibody data for demonstration with DyAb model.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create random heavy and light chain sequences\n",
    "    amino_acids = list(\"ARNDCEQGHILKMFPSTWYV\")\n",
    "    \n",
    "    heavy_length = np.random.randint(110, 130, 1)[0]\n",
    "    light_length = np.random.randint(105, 115, 1)[0]\n",
    "    \n",
    "    heavy_chains = []\n",
    "    light_chains = []\n",
    "    pkd_values = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        heavy = ''.join(np.random.choice(amino_acids, heavy_length))\n",
    "        light = ''.join(np.random.choice(amino_acids, light_length))\n",
    "        heavy_chains.append(heavy)\n",
    "        light_chains.append(light)\n",
    "\n",
    "        # Create synthetic pKD values (binding affinity)\n",
    "        # For this toy example, we'll make more A's in the hc and M's in the lc \n",
    "        # mean high affinity\n",
    "        pkd_values.append((heavy.count('A') + light.count('M'))/2 + np.random.normal(0, 0.5, 1)[0])\n",
    "\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'fv_heavy': heavy_chains,\n",
    "        'fv_light': light_chains,\n",
    "        'pKD': pkd_values\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate sample data\n",
    "df = generate_sample_data(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up the DyAbDataFrameLightningDataModule\n",
    "\n",
    "The DyAb model expects data in the form of paired sequences with their corresponding\n",
    "target values, where during training it learns to predict the differences between pairs.\n",
    "\n",
    "Let's set up the data module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damianit/Documents/lobster-docs/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence1 shape: (16,)\n",
      "Sequence2 shape: (16,)\n",
      "Target1 shape: torch.Size([16])\n",
      "Target2 shape: torch.Size([16])\n",
      "Target difference (what model will predict): 0.117531120992096\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer transform\n",
    "tokenizer = AminoAcidTokenizerFast()\n",
    "transform_fn = TokenizerTransform(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "# Initialize the DyAb datamodule\n",
    "dyab_datamodule = DyAbDataFrameLightningDataModule(\n",
    "    data=df,\n",
    "    remove_nulls=True,\n",
    "    transform_fn=transform_fn,\n",
    "    lengths=[0.8, 0.1, 0.1],  # Train, val, test split\n",
    "    batch_size=16,\n",
    "    seed=SEED,\n",
    "    num_workers=4,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# Set up the datamodule\n",
    "dyab_datamodule.prepare_data()\n",
    "dyab_datamodule.setup(stage=\"fit\")\n",
    "\n",
    "# Let's examine what a batch from the dataloader looks like\n",
    "train_dataloader = dyab_datamodule.train_dataloader()\n",
    "for batch in train_dataloader:\n",
    "    sequence1, sequence2, target1, target2 = batch\n",
    "    print(\"Sequence1 shape:\", np.array(sequence1[0]).shape)  # First element of the tuple for heavy chain\n",
    "    print(\"Sequence2 shape:\", np.array(sequence2[0]).shape)  # First element of the tuple for heavy chain\n",
    "    print(\"Target1 shape:\", target1.shape)\n",
    "    print(\"Target2 shape:\", target2.shape)\n",
    "    print(\"Target difference (what model will predict):\", (target1 - target2).mean().item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using the DyAb Model\n",
    "\n",
    "Now we'll initialize and train the DyAb model. We'll use the FlexBERT architecture\n",
    "which is designed to handle protein sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmForMaskedLM: ['esm.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing EsmForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/damianit/Documents/lobster-docs/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/damianit/Documents/lobster-docs/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/damianit/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "# Import the DyAb model\n",
    "from lobster.model import DyAbModel\n",
    "\n",
    "# Initialize the model\n",
    "dyab_model = DyAbModel(\n",
    "    model_name=\"esm2_t6_8M_UR50D\",\n",
    "    embedding_img_size=224,\n",
    "    diff_channel_0=\"diff\",\n",
    "    diff_channel_1=\"diff\",\n",
    "    diff_channel_2=\"diff\"\n",
    ")\n",
    "\n",
    "# Define callbacks for training\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath='checkpoints/',\n",
    "        filename='dyab-{epoch:02d}-{val_loss:.4f}',\n",
    "        save_top_k=3,\n",
    "        monitor='val/loss',\n",
    "        mode='min'\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val/loss',\n",
    "        patience=10,\n",
    "        mode='min'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=callbacks,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model\n",
    "\n",
    "Now let's train the DyAb model using our datamodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name           | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0  | _resize        | Resize            | 0      | train\n",
      "1  | model          | LobsterPMLM       | 7.5 M  | train\n",
      "2  | train_r2score  | R2Score           | 0      | train\n",
      "3  | val_r2score    | R2Score           | 0      | train\n",
      "4  | test_r2score   | R2Score           | 0      | train\n",
      "5  | train_mae      | MeanAbsoluteError | 0      | train\n",
      "6  | val_mae        | MeanAbsoluteError | 0      | train\n",
      "7  | test_mae       | MeanAbsoluteError | 0      | train\n",
      "8  | train_spearman | SpearmanCorrCoef  | 0      | train\n",
      "9  | val_spearman   | SpearmanCorrCoef  | 0      | train\n",
      "10 | test_spearman  | SpearmanCorrCoef  | 0      | train\n",
      "11 | resnet         | ResNet            | 11.2 M | train\n",
      "12 | loss           | MSELoss           | 0      | train\n",
      "--------------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "7.5 M     Non-trainable params\n",
      "18.7 M    Total params\n",
      "74.758    Total estimated model params size (MB)\n",
      "81        Modules in train mode\n",
      "123       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damianit/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdyab_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdyab_datamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     51\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    592\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1054\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1054\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1083\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:145\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:411\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    409\u001b[0m     batch \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mconvert_input(batch)\n\u001b[1;32m    410\u001b[0m     batch \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39m_on_before_batch_transfer(batch, dataloader_idx\u001b[38;5;241m=\u001b[39mdataloader_idx)\n\u001b[0;32m--> 411\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_to_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# the `_step` methods don't take a batch_idx when `dataloader_iter` is used, but all other hooks still do,\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# so we need different kwargs\u001b[39;00m\n\u001b[1;32m    415\u001b[0m hook_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_kwargs(\n\u001b[1;32m    416\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_sequential \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    417\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:328\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 328\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    331\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:278\u001b[0m, in \u001b[0;36mStrategy.batch_to_device\u001b[0;34m(self, batch, device, dataloader_idx)\u001b[0m\n\u001b[1;32m    276\u001b[0m device \u001b[38;5;241m=\u001b[39m device \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_device\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_batch_transfer_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m move_data_to_device(batch, device)\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/core/module.py:352\u001b[0m, in \u001b[0;36mLightningModule._apply_batch_transfer_handler\u001b[0;34m(self, batch, device, dataloader_idx)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply_batch_transfer_handler\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m, batch: Any, device: Optional[torch\u001b[38;5;241m.\u001b[39mdevice] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dataloader_idx: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    350\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    351\u001b[0m     device \u001b[38;5;241m=\u001b[39m device \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 352\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransfer_batch_to_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_after_batch_transfer\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch, dataloader_idx)\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/core/module.py:341\u001b[0m, in \u001b[0;36mLightningModule._call_batch_hook\u001b[0;34m(self, hook_name, *args)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m         trainer_method \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_lightning_datamodule_hook\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, hook_name)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hook(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:176\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    179\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/pytorch/core/hooks.py:611\u001b[0m, in \u001b[0;36mDataHooks.transfer_batch_to_device\u001b[0;34m(self, batch, device, dataloader_idx)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransfer_batch_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: Any, device: torch\u001b[38;5;241m.\u001b[39mdevice, dataloader_idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    565\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m    structure.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m \n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmove_data_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/fabric/utilities/apply_func.py:110\u001b[0m, in \u001b[0;36mmove_data_to_device\u001b[0;34m(batch, device)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# user wrongly implemented the `_TransferableDataType` and forgot to return `self`.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_TransferableDataType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_to\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py:74\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: function(v, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# slow path for everything else\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_to_collection_slow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrong_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrong_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_none\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_frozen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_frozen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py:127\u001b[0m, in \u001b[0;36m_apply_to_collection_slow\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m--> 127\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[43m_apply_to_collection_slow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrong_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrong_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_none\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_frozen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_frozen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m include_none \u001b[38;5;129;01mor\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m         out\u001b[38;5;241m.\u001b[39mappend(v)\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py:98\u001b[0m, in \u001b[0;36m_apply_to_collection_slow\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply_to_collection_slow\u001b[39m(\n\u001b[1;32m     87\u001b[0m     data: Any,\n\u001b[1;32m     88\u001b[0m     dtype: Union[\u001b[38;5;28mtype\u001b[39m, Any, \u001b[38;5;28mtuple\u001b[39m[Union[\u001b[38;5;28mtype\u001b[39m, Any]]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# Breaking condition\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype) \u001b[38;5;129;01mand\u001b[39;00m (wrong_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> 98\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     elem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/lobster-docs/.venv/lib/python3.12/site-packages/lightning/fabric/utilities/apply_func.py:104\u001b[0m, in \u001b[0;36mmove_data_to_device.<locals>.batch_to\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _BLOCKING_DEVICE_TYPES:\n\u001b[1;32m    103\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon_blocking\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m data_output \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_output\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.fit(dyab_model, datamodule=dyab_datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the Model\n",
    "\n",
    "Let's evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_results = trainer.test(dyab_model, datamodule=dyab_datamodule)\n",
    "print(f\"Test results: {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings\n",
    "for emb in dyab_model.embedding_cache:\n",
    "    print(dyab_model.embedding_cache[emb].shape)\n",
    "    if dyab_model.embedding_cache[emb].dim() == 3:\n",
    "        plt.imshow(dyab_model.embedding_cache[emb][0].cpu())\n",
    "    else:\n",
    "        plt.imshow(dyab_model.embedding_cache[emb].cpu())\n",
    "    plt.title('antiberty_emb')\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "for batch in dyab_datamodule.train_dataloader():\n",
    "    sequences1, sequences2, y1, y2 = batch\n",
    "    actual_diff = (y1 - y2).float()\n",
    "\n",
    "    if len(sequences1) == 2:  # concat multiple chains\n",
    "        sequences1 = [s1 + \".\" + s2 for s1, s2 in zip(*sequences1)]\n",
    "    if len(sequences2) == 2:  # concat multiple chains\n",
    "        sequences2 = [s1 + \".\" + s2 for s1, s2 in zip(*sequences2)]\n",
    "\n",
    "    for seq1, seq2 in zip(sequences1, sequences2):\n",
    "        if seq1 not in dyab_model.embedding_cache:\n",
    "            with torch.inference_mode():\n",
    "                hidden_states = dyab_model.model.sequences_to_latents([seq1])[-2].to(device).float()\n",
    "            dyab_model.embedding_cache[seq1] = hidden_states\n",
    "        if seq2 not in dyab_model.embedding_cache:\n",
    "            with torch.inference_mode():\n",
    "                hidden_states = dyab_model.model.sequences_to_latents([seq2])[-2].to(device).float()\n",
    "            dyab_model.embedding_cache[seq2] = hidden_states\n",
    "\n",
    "        embeddings1 = torch.concat([dyab_model.embedding_cache[seq] for seq in sequences1], dim=0).to(dyab_model.device)\n",
    "        embeddings2 = torch.concat([dyab_model.embedding_cache[seq] for seq in sequences2], dim=0).to(dyab_model.device)\n",
    "\n",
    "    embedding_image = dyab_model._resize_embeddings(embeddings1, embeddings2)\n",
    "\n",
    "    numpy_img = embedding_image[0].cpu().numpy()\n",
    "    numpy_img = np.transpose(numpy_img, (1, 2, 0))\n",
    "\n",
    "    plt.imshow(numpy_img)\n",
    "    plt.title(\"embedding_diff\")\n",
    "    plt.show()\n",
    "\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize an embedding\n",
    "\n",
    "DyAb uses embeddings generated by another protein language model to predict the difference between two\n",
    "sequences. Let's visualize one of these embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform Inference on New Sequences\n",
    "\n",
    "Now let's demonstrate how to use the trained model for inference on new antibody sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binding_differences(model, new_data):\n",
    "    \"\"\"\n",
    "    Predict binding affinity differences for pairs of antibody sequences.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained DyAb model\n",
    "        new_data: DataFrame with fv_heavy, fv_light, pKD columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predicted differences\n",
    "    \"\"\"\n",
    "    # Create a datamodule for inference\n",
    "    inference_datamodule = DyAbDataFrameLightningDataModule(\n",
    "        data=new_data,\n",
    "        remove_nulls=True,\n",
    "        transform_fn=transform_fn,\n",
    "        lengths=[0, 0, 1],  # All data for inference\n",
    "        batch_size=16,\n",
    "        seed=SEED,\n",
    "        num_workers=4,\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    # Set up the datamodule\n",
    "    inference_datamodule.prepare_data()\n",
    "    inference_datamodule.setup(stage=\"predict\")\n",
    "    \n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    sequence1_data = []\n",
    "    sequence2_data = []\n",
    "    actual_diffs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(inference_datamodule.predict_dataloader()):\n",
    "            sequence1, sequence2, target1, target2 = batch\n",
    "            \n",
    "            preds, _ = model.predict_step(batch, idx)\n",
    "\n",
    "            # y_hat, _ = trainer.predict(model, inference_datamodule)[idx]\n",
    "\n",
    "            # Process through the model\n",
    "            # pred_diff = model(sequence1, sequence2)\n",
    "            actual_diff = (target1 - target2).cpu().numpy()\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            actual_diffs.extend(actual_diff)\n",
    "            \n",
    "            # Store sequence information for reference\n",
    "            for i in range(len(target1)):\n",
    "                sequence1_data.append({\n",
    "                    \"heavy\": sequence1[0][i],\n",
    "                    \"light\": sequence1[1][i],\n",
    "                    \"target\": target1[i].item()\n",
    "                })\n",
    "                sequence2_data.append({\n",
    "                    \"heavy\": sequence2[0][i],\n",
    "                    \"light\": sequence2[1][i],\n",
    "                    \"target\": target2[i].item()\n",
    "                })            \n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        \"Predicted_Difference\": predictions,\n",
    "        \"Actual_Difference\": actual_diffs,\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Generate some new sequences for inference\n",
    "new_data = generate_sample_data(100)\n",
    "\n",
    "# Predict binding affinity differences\n",
    "results_df = predict_binding_differences(dyab_model, new_data)\n",
    "\n",
    "# Display results\n",
    "print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Predictions vs. Actual Differences\n",
    "\n",
    "Let's visualize how well our model's predictions match the actual differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted differences\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(results_df[\"Actual_Difference\"], results_df[\"Predicted_Difference\"], alpha=0.7)\n",
    "plt.plot(\n",
    "    [min(results_df[\"Actual_Difference\"]), max(results_df[\"Actual_Difference\"])], \n",
    "    [min(results_df[\"Actual_Difference\"]), max(results_df[\"Actual_Difference\"])], \n",
    "    'r--'\n",
    ")\n",
    "plt.xlabel('Actual Difference')\n",
    "plt.ylabel('Predicted Difference')\n",
    "plt.title('Predicted vs. Actual Affinity Differences')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate R² score\n",
    "r2 = r2_score(results_df[\"Actual_Difference\"], results_df[\"Predicted_Difference\"])\n",
    "print(f\"R² score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using the Model for Ranking Antibodies\n",
    "\n",
    "One valuable application of the DyAb model is to rank antibodies based on predicted affinities.\n",
    "Let's demonstrate how to use the model for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_antibodies(model, antibody_pool, reference_antibody):\n",
    "    \"\"\"\n",
    "    Rank a pool of antibodies against a reference antibody based on predicted binding affinity.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained DyAb model\n",
    "        antibody_pool: DataFrame with fv_heavy, fv_light, pKD columns for candidate antibodies\n",
    "        reference_antibody: Dict with fv_heavy, fv_light for the reference antibody\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with ranked antibodies\n",
    "    \"\"\"\n",
    "    # Create pairs of reference and candidate antibodies\n",
    "    pairs_data = []\n",
    "    \n",
    "    for _, candidate in antibody_pool.iterrows():\n",
    "        pairs_data.append({\n",
    "            \"fv_heavy\": reference_antibody[\"fv_heavy\"],\n",
    "            \"fv_light\": reference_antibody[\"fv_light\"],\n",
    "            \"pKD\": 0.0,  # Dummy value for reference\n",
    "            \"candidate_heavy\": candidate[\"fv_heavy\"],\n",
    "            \"candidate_light\": candidate[\"fv_light\"],\n",
    "            \"candidate_pKD\": candidate[\"pKD\"]\n",
    "        })\n",
    "    \n",
    "    pairs_df = pd.DataFrame(pairs_data)\n",
    "    \n",
    "    # Create a combined dataset for DyAb format\n",
    "    # We need to duplicate each row and swap the order to get predictions in both directions\n",
    "    combined_data = []\n",
    "    \n",
    "    for _, row in pairs_df.iterrows():\n",
    "        # Reference first, candidate second\n",
    "        combined_data.append({\n",
    "            \"fv_heavy\": row[\"fv_heavy\"],\n",
    "            \"fv_light\": row[\"fv_light\"],\n",
    "            \"pKD\": row[\"pKD\"]\n",
    "        })\n",
    "        \n",
    "        # Candidate first, reference second\n",
    "        combined_data.append({\n",
    "            \"fv_heavy\": row[\"candidate_heavy\"],\n",
    "            \"fv_light\": row[\"candidate_light\"],\n",
    "            \"pKD\": row[\"candidate_pKD\"]\n",
    "        })\n",
    "    \n",
    "    combined_df = pd.DataFrame(combined_data)\n",
    "    \n",
    "    # Create a datamodule\n",
    "    inference_datamodule = DyAbDataFrameLightningDataModule(\n",
    "        data=combined_df,\n",
    "        remove_nulls=True,\n",
    "        transform_fn=transform_fn,\n",
    "        lengths=[0, 0, 1],  # All data for inference\n",
    "        batch_size=16,\n",
    "        seed=SEED,\n",
    "        num_workers=4,\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    # Set up the datamodule\n",
    "    inference_datamodule.prepare_data()\n",
    "    inference_datamodule.setup(stage=\"predict\")\n",
    "    \n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in inference_datamodule.predict_dataloader():\n",
    "            # pred_diff = model(sequence1, sequence2)\n",
    "            preds = dyab_model.predict(batch)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Process predictions (every second prediction is for ref-candidate pair)\n",
    "    ref_candidate_preds = all_preds[::2]\n",
    "    \n",
    "    # Add predictions to the original dataframe\n",
    "    for i, pred in enumerate(ref_candidate_preds):\n",
    "        pairs_df.loc[i, \"predicted_diff_ref_candidate\"] = pred\n",
    "    \n",
    "    # Sort by prediction (higher predicted difference means candidate is better than reference)\n",
    "    ranked_df = pairs_df.sort_values(by=\"predicted_diff_ref_candidate\", ascending=False)\n",
    "    \n",
    "    # Add rank column\n",
    "    ranked_df[\"rank\"] = range(1, len(ranked_df) + 1)\n",
    "    \n",
    "    return ranked_df[[\"rank\", \"candidate_heavy\", \"candidate_light\", \"candidate_pKD\", \"predicted_diff_ref_candidate\"]]\n",
    "\n",
    "# Generate a pool of candidate antibodies\n",
    "candidate_pool = generate_sample_data(100)\n",
    "\n",
    "# Select a reference antibody\n",
    "reference_antibody = {\n",
    "    \"fv_heavy\": df.iloc[0][\"fv_heavy\"],\n",
    "    \"fv_light\": df.iloc[0][\"fv_light\"]\n",
    "}\n",
    "\n",
    "# Rank antibodies\n",
    "ranked_candidates = rank_antibodies(dyab_model, candidate_pool, reference_antibody)\n",
    "\n",
    "# Display top 10 ranked antibodies\n",
    "print(\"Top 10 Ranked Antibodies:\")\n",
    "print(ranked_candidates.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Saving and Loading the Model\n",
    "\n",
    "Demonstrating how to save and load a trained DyAb model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = \"dyab_model.ckpt\"\n",
    "trainer.save_checkpoint(model_path)\n",
    "\n",
    "# Load the model\n",
    "loaded_model = DyAbModel.load_from_checkpoint(model_path)\n",
    "\n",
    "# Verify the loaded model\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a batch from the dataloader\n",
    "    for batch in dyab_datamodule.val_dataloader():\n",
    "        sequence1, sequence2, target1, target2 = batch\n",
    "        \n",
    "        # Make predictions with both models\n",
    "        # original_pred = dyab_model(sequence1, sequence2)\n",
    "        with torch.inference_mode():\n",
    "            output = dyab_model._compute_loss(batch)\n",
    "        assert output is not None\n",
    "        _, original_pred, _ = output\n",
    "        \n",
    "        # loaded_pred = loaded_model(sequence1, sequence2)\n",
    "        with torch.inference_mode():\n",
    "            output = loaded_model._compute_loss(batch)\n",
    "        assert output is not None\n",
    "        _, loaded_pred, _ = output\n",
    "        \n",
    "        # Verify they give the same predictions\n",
    "        print(\"Original model prediction mean:\", original_pred.mean().item())\n",
    "        print(\"Loaded model prediction mean:\", loaded_pred.mean().item())\n",
    "        \n",
    "        # Check if predictions are identical\n",
    "        is_identical = torch.allclose(original_pred, loaded_pred, atol=1e-5)\n",
    "        print(f\"Models give identical predictions: {is_identical}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this tutorial, we've demonstrated how to:\n",
    "\n",
    "1. Prepare antibody data for the DyAb model\n",
    "2. Train a DyAb model using DataFrameLightningDataModule\n",
    "3. Evaluate the model's performance\n",
    "4. Use the model for inference and ranking of antibodies\n",
    "5. Save and load the model\n",
    "\n",
    "The DyAb model is particularly useful for learning pairwise relationships between sequences,\n",
    "which makes it valuable for tasks like antibody optimization, where you want to predict if\n",
    "a modification to a sequence will improve its properties."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
