{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ba380f",
   "metadata": {},
   "source": [
    "# Embedding Extraction with LBSTER\n",
    "\n",
    "This tutorial demonstrates how to extract embeddings from protein sequences using pre-trained LBSTER models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d14cdb",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, make sure you have LBSTER installed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d81f2",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "We'll start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a5a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lobster.model import LobsterPMLM, LobsterCBMPMLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba65a4e",
   "metadata": {},
   "source": [
    "## Loading a Pre-trained Model\n",
    "\n",
    "LBSTER provides several pre-trained models. Let's load a masked language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1894247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model to use\n",
    "model_name = \"asalam91/lobster_24M\"  # 24M parameter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = LobsterPMLM(model_name)\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de853e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1662b",
   "metadata": {},
   "source": [
    "## Sample Protein Sequences\n",
    "\n",
    "Let's define some sample protein sequences to extract embeddings from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcdf92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    \"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSHGSAQVKGHGKKVADALTNAVAHVDDMPNALSALSDLHAHKLRVDPVNFKLLSHCLLVTLAAHLPAEFTPAVHASLDKFLASVSTVLTSKYR\",  # Hemoglobin alpha\n",
    "    \"MVHLTPEEKSAVTALWGKVNVDEVGGEALGRLLVVYPWTQRFFESFGDLSTPDAVMGNPKVKAHGKKVLGAFSDGLAHLDNLKGTFATLSELHCDKLHVDPENFRLLGNVLVCVLAHHFGKEFTPPVQAAYQKVVAGVANALAHKYH\",  # Hemoglobin beta\n",
    "    \"MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSELDKAIGRNTNGVITKDEAEKLFNQDVDAAVRGILRNAKLKPVYDSLDAVRRAALINMVFQMGETGVAGFTNSLRMLQQKRWDEAAVNLAKSRWYNQTPNRAKRVITTFRTGTWDAYKNL\",  # T4 Lysozyme\n",
    "    \"MEAPAAGAAPPPGPALGNGVAGAGGEAAAAPGGGGEAPARKRGRPGGDNHGPGREARDGPRERLGAGPADAGPGAPGSQHPGGRGRGGGPGLSTLPGGGPGPGGFGPLGFPMRGRGGPGPGGFGPRGGPGAAGFPTRGRGGGPGPDGF\",  # Heterogeneous Nuclear Ribonucleoprotein A1\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff5f76f",
   "metadata": {},
   "source": [
    "## Extracting Embeddings\n",
    "\n",
    "Now we'll extract embeddings from these sequences using our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e178f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    # Get embeddings for each sequence\n",
    "    embeddings = []\n",
    "    for seq in sequences:\n",
    "        # Tokenize and process the sequence\n",
    "        tokens = model.tokenizer(seq, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Get the embedding (using the [CLS] token representation)\n",
    "        outputs = model.model(\n",
    "            input_ids=tokens[\"input_ids\"],\n",
    "            attention_mask=tokens[\"attention_mask\"]\n",
    "        )\n",
    "        \n",
    "        # Extract the [CLS] token embedding\n",
    "        cls_embedding = outputs[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(cls_embedding.squeeze())\n",
    "    \n",
    "    # Convert list to numpy array\n",
    "    embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dca865",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cace2c",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings\n",
    "\n",
    "Let's visualize the embeddings using PCA to reduce dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19699ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions with PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d61fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels\n",
    "for i, seq_name in enumerate([\"Hemoglobin α\", \"Hemoglobin β\", \"T4 Lysozyme\", \"hnRNP A1\"]):\n",
    "    plt.annotate(seq_name, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"PCA of Protein Embeddings\", fontsize=14)\n",
    "plt.xlabel(\"PC1\", fontsize=12)\n",
    "plt.ylabel(\"PC2\", fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b52a9b",
   "metadata": {},
   "source": [
    "## Using Concept Bottleneck Models\n",
    "\n",
    "LBSTER also provides concept bottleneck models that can provide interpretable concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b319d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a concept bottleneck model\n",
    "cb_model_name = \"asalam91/cb_lobster_24M\"\n",
    "cb_model = LobsterCBMPMLM(cb_model_name)\n",
    "cb_model.eval()\n",
    "cb_model = cb_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562800b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract concepts\n",
    "concepts = []\n",
    "with torch.no_grad():\n",
    "    for seq in sequences:\n",
    "        # Tokenize and process the sequence\n",
    "        tokens = cb_model.tokenizer(seq, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Get concepts\n",
    "        outputs = cb_model.model(\n",
    "            input_ids=tokens[\"input_ids\"],\n",
    "            attention_mask=tokens[\"attention_mask\"],\n",
    "            inference=True\n",
    "        )\n",
    "        \n",
    "        # Extract the concepts\n",
    "        seq_concepts = outputs[\"concepts\"].cpu().numpy().squeeze()\n",
    "        concepts.append(seq_concepts)\n",
    "    \n",
    "    # Convert list to numpy array\n",
    "    concepts = np.array(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Concepts shape: {concepts.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468616d2",
   "metadata": {},
   "source": [
    "## Analyzing Top Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 5 concepts for each sequence\n",
    "concept_names = cb_model.concept_names[:concepts.shape[1]]  # Get the concept names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cdf013",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seq_name in enumerate([\"Hemoglobin α\", \"Hemoglobin β\", \"T4 Lysozyme\", \"hnRNP A1\"]):\n",
    "    # Get the top 5 concept indices for this sequence\n",
    "    top_concept_indices = np.argsort(concepts[i])[-5:][::-1]\n",
    "    \n",
    "    # Display the top concepts and their values\n",
    "    print(f\"\\nTop concepts for {seq_name}:\")\n",
    "    for idx in top_concept_indices:\n",
    "        print(f\"  {concept_names[idx]}: {concepts[i][idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef89ae",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've demonstrated how to:\n",
    "\n",
    "1. Load pre-trained LBSTER models\n",
    "2. Extract embeddings from protein sequences\n",
    "3. Visualize these embeddings using PCA\n",
    "4. Extract and analyze interpretable concepts using the concept bottleneck models\n",
    "\n",
    "These embeddings can be used for various downstream tasks such as clustering, classification, or visualization of protein sequences."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
