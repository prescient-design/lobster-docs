{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ba380f",
   "metadata": {},
   "source": [
    "# Embedding Extraction with LBSTER\n",
    "\n",
    "This tutorial demonstrates how to extract embeddings from protein sequences using pre-trained LBSTER models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d14cdb",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, make sure you have LBSTER installed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d81f2",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "We'll start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c0b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69a5a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ncfrey/Documents/GitHub/lobster-docs/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lobster.model import LobsterPMLM, LobsterCBMPMLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba65a4e",
   "metadata": {},
   "source": [
    "## Loading a Pre-trained Model\n",
    "\n",
    "LBSTER provides several pre-trained models. Let's load a masked language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1894247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model to use\n",
    "model_name = \"asalam91/lobster_24M\"  # 24M parameter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "529b852e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LobsterPMLM(\n",
       "  (_transform_fn): PmlmTokenizerTransform()\n",
       "  (model): LMBaseForMaskedLM(\n",
       "    (LMBase): LMBaseModel(\n",
       "      (embeddings): LMBaseEmbeddings(\n",
       "        (word_embeddings): Embedding(32, 408, padding_idx=1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (position_embeddings): Embedding(512, 408, padding_idx=1)\n",
       "      )\n",
       "      (encoder): LMBaseEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-9): 10 x LMBaseLayer(\n",
       "            (attention): LMBaseAttention(\n",
       "              (self): LMBaseSelfAttention(\n",
       "                (query): Linear(in_features=408, out_features=408, bias=False)\n",
       "                (key): Linear(in_features=408, out_features=408, bias=False)\n",
       "                (value): Linear(in_features=408, out_features=408, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): LMBaseSelfOutput(\n",
       "                (dense): Linear(in_features=408, out_features=408, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((408,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): LMBaseIntermediate(\n",
       "              (dense): Linear(in_features=408, out_features=2048, bias=False)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): LMBaseOutput(\n",
       "              (dense): Linear(in_features=2048, out_features=408, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((408,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm_after): LayerNorm((408,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): LMBaseLMHead(\n",
       "      (dense): Linear(in_features=408, out_features=408, bias=True)\n",
       "      (layer_norm): LayerNorm((408,), eps=1e-12, elementwise_affine=True)\n",
       "      (decoder): Linear(in_features=408, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = LobsterPMLM(model_name)\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de853e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1662b",
   "metadata": {},
   "source": [
    "## Sample Protein Sequences\n",
    "\n",
    "Let's define some sample protein sequences to extract embeddings from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebcdf92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    \"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSHGSAQVKGHGKKVADALTNAVAHVDDMPNALSALSDLHAHKLRVDPVNFKLLSHCLLVTLAAHLPAEFTPAVHASLDKFLASVSTVLTSKYR\",  # Hemoglobin alpha\n",
    "    \"MVHLTPEEKSAVTALWGKVNVDEVGGEALGRLLVVYPWTQRFFESFGDLSTPDAVMGNPKVKAHGKKVLGAFSDGLAHLDNLKGTFATLSELHCDKLHVDPENFRLLGNVLVCVLAHHFGKEFTPPVQAAYQKVVAGVANALAHKYH\",  # Hemoglobin beta\n",
    "    \"MNIFEMLRIDEGLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSELDKAIGRNTNGVITKDEAEKLFNQDVDAAVRGILRNAKLKPVYDSLDAVRRAALINMVFQMGETGVAGFTNSLRMLQQKRWDEAAVNLAKSRWYNQTPNRAKRVITTFRTGTWDAYKNL\",  # T4 Lysozyme\n",
    "    \"MEAPAAGAAPPPGPALGNGVAGAGGEAAAAPGGGGEAPARKRGRPGGDNHGPGREARDGPRERLGAGPADAGPGAPGSQHPGGRGRGGGPGLSTLPGGGPGPGGFGPLGFPMRGRGGPGPGGFGPRGGPGAAGFPTRGRGGGPGPDGF\",  # Heterogeneous Nuclear Ribonucleoprotein A1\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff5f76f",
   "metadata": {},
   "source": [
    "## Extracting Embeddings\n",
    "\n",
    "Now we'll extract embeddings from these sequences using our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0e178f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m     11\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39mtokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     12\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mtokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Extract the [CLS] token embedding\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     cls_embedding \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     17\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(cls_embedding\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Convert list to numpy array\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/lobster-docs/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:435\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_dict[k]\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# Turn off gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    # Get embeddings for each sequence\n",
    "    embeddings = []\n",
    "    for seq in sequences:\n",
    "        # Tokenize and process the sequence\n",
    "        tokens = model.tokenizer(seq, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Get the embedding (using the [CLS] token representation)\n",
    "        outputs = model.model(\n",
    "            input_ids=tokens[\"input_ids\"],\n",
    "            attention_mask=tokens[\"attention_mask\"]\n",
    "        )\n",
    "        \n",
    "        # Extract the [CLS] token embedding\n",
    "        cls_embedding = outputs[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(cls_embedding.squeeze())\n",
    "    \n",
    "    # Convert list to numpy array\n",
    "    embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dca865",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cace2c",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings\n",
    "\n",
    "Let's visualize the embeddings using PCA to reduce dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19699ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions with PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d61fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels\n",
    "for i, seq_name in enumerate([\"Hemoglobin α\", \"Hemoglobin β\", \"T4 Lysozyme\", \"hnRNP A1\"]):\n",
    "    plt.annotate(seq_name, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"PCA of Protein Embeddings\", fontsize=14)\n",
    "plt.xlabel(\"PC1\", fontsize=12)\n",
    "plt.ylabel(\"PC2\", fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b52a9b",
   "metadata": {},
   "source": [
    "## Using Concept Bottleneck Models\n",
    "\n",
    "LBSTER also provides concept bottleneck models that can provide interpretable concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b319d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a concept bottleneck model\n",
    "cb_model_name = \"asalam91/cb_lobster_24M\"\n",
    "cb_model = LobsterCBMPMLM(cb_model_name)\n",
    "cb_model.eval()\n",
    "cb_model = cb_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562800b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract concepts\n",
    "concepts = []\n",
    "with torch.no_grad():\n",
    "    for seq in sequences:\n",
    "        # Tokenize and process the sequence\n",
    "        tokens = cb_model.tokenizer(seq, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Get concepts\n",
    "        outputs = cb_model.model(\n",
    "            input_ids=tokens[\"input_ids\"],\n",
    "            attention_mask=tokens[\"attention_mask\"],\n",
    "            inference=True\n",
    "        )\n",
    "        \n",
    "        # Extract the concepts\n",
    "        seq_concepts = outputs[\"concepts\"].cpu().numpy().squeeze()\n",
    "        concepts.append(seq_concepts)\n",
    "    \n",
    "    # Convert list to numpy array\n",
    "    concepts = np.array(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Concepts shape: {concepts.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468616d2",
   "metadata": {},
   "source": [
    "## Analyzing Top Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 5 concepts for each sequence\n",
    "concept_names = cb_model.concept_names[:concepts.shape[1]]  # Get the concept names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cdf013",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seq_name in enumerate([\"Hemoglobin α\", \"Hemoglobin β\", \"T4 Lysozyme\", \"hnRNP A1\"]):\n",
    "    # Get the top 5 concept indices for this sequence\n",
    "    top_concept_indices = np.argsort(concepts[i])[-5:][::-1]\n",
    "    \n",
    "    # Display the top concepts and their values\n",
    "    print(f\"\\nTop concepts for {seq_name}:\")\n",
    "    for idx in top_concept_indices:\n",
    "        print(f\"  {concept_names[idx]}: {concepts[i][idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef89ae",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've demonstrated how to:\n",
    "\n",
    "1. Load pre-trained LBSTER models\n",
    "2. Extract embeddings from protein sequences\n",
    "3. Visualize these embeddings using PCA\n",
    "4. Extract and analyze interpretable concepts using the concept bottleneck models\n",
    "\n",
    "These embeddings can be used for various downstream tasks such as clustering, classification, or visualization of protein sequences."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}